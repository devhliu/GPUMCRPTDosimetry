# Development Record: Phases 10 & 11

This document records the development process for Phases 10 and 11, which introduced a more advanced particle bank architecture and a highly optimized, GPU-native compaction pipeline.

## Phase 10: Bank Architecture and Atomic Relaxation

Phase 10 represented a major architectural shift, moving from separate, simple particle queues to a more robust "bank" architecture. This was a prerequisite for implementing more complex physics, particularly atomic relaxation.

### Key Developments in Phase 10:
1.  **Particle Bank Architecture:**
    *   The particle data was organized into "banks" (e.g., `photon_bank`, `electron_bank`).
    *   These banks are large, pre-allocated buffers that store all particle attributes in a Structure-of-Arrays (SoA) layout.
    *   The banks were upgraded to store additional information required for advanced physics, including the **energy bin (`ebin`)** and the full **Philox RNG state**.

2.  **Atomic Appends:**
    *   New particles are added to the banks using an "atomic append" mechanism. This involves atomically incrementing a global counter for the bank and then writing the new particle's data to the end of the bank. This is a highly efficient, lock-free way to grow the particle queues on the GPU.

3.  **Vacancy and Relaxation Physics:**
    *   A new **vacancy bank** was created to track atomic vacancies created by the photoelectric effect.
    *   A dispatch and kernel for **atomic relaxation** was implemented. This kernel processes the vacancy bank and simulates the subsequent cascade of fluorescence X-rays and Auger electrons.
    *   Particles generated by the relaxation cascade are added to the appropriate photon or electron banks for further transport. Energy from particles below the transport cutoff is deposited locally.

4.  **Fully GPU-Resident Control Flow:**
    *   The combination of the bank architecture and atomic appends allowed for a fully GPU-resident control flow for the photoelectric and relaxation processes, with no CPU synchronization (`.item()`) in the hot loop.

## Phase 11: Lazy Synchronization and GPU-Native Compaction

Phase 11 focused on optimizing the most significant remaining bottleneck: the compaction process and the associated CPU-GPU synchronization. The goal was to create a "lazy sync" pipeline where the CPU synchronizes with the GPU as infrequently as possible.

### Key Developments in Phase 11:
1.  **Lazy Synchronization Policy:**
    *   A "one sync per step" policy was adopted. At the beginning of each simulation step, the CPU performs a single read of the global particle counters to get the number of "dirty" particles (the maximum number of potentially active particles).
    *   All kernels in the step are then launched with this "dirty" size (oversubscribed). The kernels themselves have an "early exit" clause to ensure that threads for inactive particles do no work.

2.  **GPU-Native Compaction:**
    *   The entire compaction process was moved to the GPU, using custom Triton kernels.
    *   The process involves:
        1.  Creating a `status == ALIVE` mask.
        2.  Performing an **exclusive scan** on the mask using a pre-allocated **Triton scan workspace** (eliminating `torch.cumsum`).
        3.  Calculating the new number of alive particles as a **GPU scalar**, without needing a CPU sync.
        4.  Packing the alive particles into a destination bank.

3.  **Ping-Pong Buffers:**
    *   To avoid memory hazards during compaction (reading from and writing to the same buffer), a **ping-pong buffer** strategy was used. The compaction process reads from a source bank and writes the compacted data to a destination bank. The bank pointers are then swapped for the next simulation step.

4.  **Validation Gates:**
    *   Before proceeding, a set of validation gates were recommended to ensure the correctness of the new architecture, including checks for determinism, energy conservation, and overflow.

By the end of Phase 11, the engine had a highly optimized, almost entirely GPU-resident simulation loop, minimizing CPU-GPU synchronization and maximizing throughput.
